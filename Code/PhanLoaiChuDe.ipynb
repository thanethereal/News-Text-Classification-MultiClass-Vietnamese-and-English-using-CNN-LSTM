{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "opZ_sUsVxd92"
      },
      "outputs": [],
      "source": [
        "from underthesea import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gd5g2SSfyc3c"
      },
      "outputs": [],
      "source": [
        "# Cài đặt một số hàm tiền xử lý văn bản cần thiết\n",
        "# !pip3 install --user underthesea\n",
        "import regex as re\n",
        "from underthesea import word_tokenize\n",
        " \n",
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_html(txt):\n",
        "    return re.sub(r'<[^>]*>', '', txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGQwYfaqyc6U",
        "outputId": "94e8a3cf-0563-48ed-d129-02a44d0ba368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tp hcm phạt người không đeo khẩu_trang nơi công_cộng người dân ở thành_phố không đeo khẩu_trang nơi công_cộng sẽ bị xử_phạt mức cao nhất 300 000 đồng từ ngày 58 yêu_cầu này được chủ_tịch ubnd thành_phố nguyễn_thành phong đưa ra tại cuộc họp ban chỉ_đạo phòng_chống dịch_bệnh covid 19 của tp hcm chiều 38 việc xử_phạt không đeo khẩu_trang nơi công_cộng được tp hcm cũng như các địa_phương khác thực_hiện từ cuối tháng 3 khi covid 19 bùng_phát tuy_nhiên sau khi hết thực_hiện cách_ly xã_hội từ ngày 234 việc đeo khẩu_trang nơi công_cộng chỉ dừng lại ở mức khuyến_cáo theo nghị_định số 1762013 người_dân không đeo khẩu_trang nơi công_cộng sẽ bị xử_phạt từ 100 000 đến 300 000 đồng trong khoảng một tháng áp_dụng trước đó tp hcm đã xử_phạt hơn 4 300 trường_hợp với gần 870 triệu đồng theo ông phong việc đeo khẩu_trang đã được khẳng_định có_thể tránh lây_lan dịch_bệnh cho người khác và bảo_vệ sức_khỏe cho người sử_dụng sở công_thương phải nắm nguồn cung_ứng_khẩu_trang chủ_động thông_báo các điểm bán để người dân dễ_dàng mua vì đã xử_phạt thì phải bảo_đảm đủ nguồn cung ông phong nói đội trật_tự đô_thị phường bến_nghé quận 1 xử_phạt người không đeo khẩu_trang trên phố đi bộ nguyễn_huệ chiều 154 ảnh quỳnh trần đội trật_tự đô_thị phường bến_nghé quận 1 xử_phạt người không đeo khẩu_trang trên phố đi bộ nguyễn_huệ chiều 154 ảnh quỳnh trần bí_thư thành_ủy nguyễn_thiện_nhân cũng cho rằng việc đeo khẩu_trang là một trong những biện_pháp cơ_bản để tránh dịch_bệnh lây_lan việc này rất dễ làm không tốn nhiều tiền nhưng nhiều nước bỏ_lơi và đã bị vỡ trận ngoài đường hiện có ít_nhất 20 người không đeo khẩu_trang người không đeo không_những tự rước bệnh vào mình mà_còn nguy_cơ lây cho người khác đeo khẩu_trang hơi cực tí thôi nhưng đi đâu cũng nên đeo để giữ an_toàn ông nhân nói và khẳng_định thành_phố bảo_đảm không thiếu khẩu_trang cho người dân chủ_tịch ubnd thành_phố nguyễn_thành phong cũng cho biết đã đồng_ý việc tái_lập các chốt kiểm_soát ở cửa_ngõ tp hcm để phòng_chống covid 19 trước đó thành_phố đã lập 62 chốt kiểm_soát hoạt_động 2424 từ ngày 44 để phòng_chống dịch lực_lượng tham_gia là công_an thành_phố sở y_tế bộ_tư_lệnh thành_phố thanh_tra giao_thông ban quản_lý an_toàn thực_phẩm quản_lý thị_trường trong đó 16 chốt chính cấp thành_phố đặt tại trạm thu phí long phước cao_tốc tp hcm long thành dầu giây cao_tốc trung_lương cầu đôi đường trần_văn_giàu đường ba làng đường xuyên á quốc_lộ 22 cầu phú_cường cầu vĩnh_bình cầu_vượt sóng_thần quốc_lộ 1 k quốc_lộ 50 quốc_lộ 1 a cầu đồng nai bến_xe miền tây bến_xe miền đông sân_bay tân_sơn nhất cảng cát_lái đến ngày 234 chính_quyền thành_phố dừng hoạt_động các chốt này vì dịch_bệnh đã được khống_chế tp hcm dừng cách_ly xã_hội theo chỉ_thị 19 của thủ_tướng sau 19 ngày hoạt_động các chốt chính đã kiểm_tra gần 270 000 xe trong đó có 235 000 ôtô gần 600 000 người được kiểm_tra y_tế đo thân_nhiệt bao_gồm cả 3 000 người nước_ngoài hơn 130 000 người được yêu_cầu khai_báo y_tế\n"
          ]
        }
      ],
      "source": [
        "def text_preprocess(document):\n",
        "    # xóa html code\n",
        "    document = remove_html(document)\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "document = \"\"\"\n",
        "TP HCM phạt người không đeo khẩu trang nơi công cộng\n",
        "Người dân ở thành phố không đeo khẩu trang nơi công cộng sẽ bị xử phạt mức cao nhất 300.000 đồng, từ ngày 5/8.\n",
        "\n",
        "Yêu cầu này được Chủ tịch UBND thành phố Nguyễn Thành Phong đưa ra tại cuộc họp Ban chỉ đạo phòng chống dịch bệnh Covid-19 của TP HCM chiều 3/8.\n",
        "\n",
        "Việc xử phạt không đeo khẩu trang nơi công cộng được TP HCM cũng như các địa phương khác thực hiện từ cuối tháng 3 khi Covid-19 bùng phát. Tuy nhiên, sau khi hết thực hiện cách ly xã hội từ ngày 23/4, việc đeo khẩu trang nơi công cộng chỉ dừng lại ở mức khuyến cáo.\n",
        "\n",
        "Theo Nghị định số 176/2013, người dân không đeo khẩu trang nơi công cộng sẽ bị xử phạt từ 100.000 đến 300.000 đồng. Trong khoảng một tháng áp dụng trước đó, TP HCM đã xử phạt hơn 4.300 trường hợp với gần 870 triệu đồng.\n",
        "\n",
        "Theo ông Phong, việc đeo khẩu trang đã được khẳng định có thể tránh lây lan dịch bệnh cho người khác và bảo vệ sức khỏe cho người sử dụng. \"Sở Công thương phải nắm nguồn cung ứng khẩu trang, chủ động thông báo các điểm bán để người dân dễ dàng mua vì đã xử phạt thì phải bảo đảm đủ nguồn cung\", ông Phong nói.\n",
        "\n",
        "Đội trật tự đô thị phường Bến Nghé, quận 1, xử phạt người không đeo khẩu trang trên phố đi bộ Nguyễn Huệ, chiều 15/4. Ảnh: Quỳnh Trần.\n",
        "Đội trật tự đô thị phường Bến Nghé, quận 1, xử phạt người không đeo khẩu trang trên phố đi bộ Nguyễn Huệ, chiều 15/4. Ảnh: Quỳnh Trần.\n",
        "\n",
        "Bí thư Thành uỷ Nguyễn Thiện Nhân cũng cho rằng việc đeo khẩu trang là một trong những biện pháp cơ bản để tránh dịch bệnh lây lan. Việc này rất dễ làm, không tốn nhiều tiền nhưng nhiều nước bỏ lơi và đã bị \"vỡ trận\".\n",
        "\n",
        "\"Ngoài đường hiện có ít nhất 20% người không đeo khẩu trang. Người không đeo không những tự rước bệnh vào mình mà còn nguy cơ lây cho người khác. Đeo khẩu trang hơi cực tí thôi nhưng đi đâu cũng nên đeo để giữ an toàn\", ông Nhân nói và khẳng định thành phố bảo đảm không thiếu khẩu trang cho người dân.\n",
        "\n",
        "Chủ tịch UBND thành phố Nguyễn Thành Phong cũng cho biết đã đồng ý việc tái lập các chốt kiểm soát ở cửa ngõ TP HCM để phòng chống Covid-19.\n",
        "\n",
        "Trước đó, thành phố đã lập 62 chốt kiểm soát, hoạt động 24/24 từ ngày 4/4 để phòng chống dịch. Lực lượng tham gia là Công an thành phố, Sở Y tế, Bộ Tư lệnh thành phố, Thanh tra giao thông, Ban Quản lý An toàn thực phẩm, quản lý thị trường.\n",
        "\n",
        "Trong đó, 16 chốt chính (cấp thành phố) đặt tại: Trạm thu phí Long Phước (cao tốc TP HCM - Long Thành - Dầu Giây), cao tốc Trung Lương, cầu Đôi (đường Trần Văn Giàu), đường Ba Làng, đường Xuyên Á (quốc lộ 22), cầu Phú Cường, cầu Vĩnh Bình, cầu vượt Sóng Thần, quốc lộ 1K, quốc lộ 50, quốc lộ 1A, cầu Đồng Nai, Bến xe Miền Tây, Bến xe miền Đông, sân bay Tân Sơn Nhất, cảng Cát Lái.\n",
        "\n",
        "Đến ngày 23/4, chính quyền thành phố dừng hoạt động các chốt này vì dịch bệnh đã được khống chế, TP HCM dừng cách ly xã hội theo Chỉ thị 19 của Thủ tướng.\n",
        "\n",
        "Sau 19 ngày hoạt động, các chốt chính đã kiểm tra gần 270.000 xe, trong đó có 235.000 ôtô; gần 600.000 người được kiểm tra y tế, đo thân nhiệt, bao gồm cả 3.000 người nước ngoài; hơn 130.000 người được yêu cầu khai báo y tế.\n",
        "\"\"\"\n",
        "\n",
        "document = text_preprocess(document)\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ1aBV9Aysr0",
        "outputId": "3477b33e-8b6d-4d68-eb5c-8c22666b60fa"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/news_categories.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12548/4002614793.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Thống kê số lượng data theo nhãn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/news_categories.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcount\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/news_categories.txt'"
          ]
        }
      ],
      "source": [
        "# Thống kê số lượng data theo nhãn\n",
        "count = {}\n",
        "for line in open('./data/news_categories.txt',encoding='utf8'):\n",
        "    key = line.split()[0]\n",
        "    count[key] = count.get(key, 0) + 1\n",
        "\n",
        "for key in count:\n",
        "    print(key, count[key])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyP5ZyaZysua",
        "outputId": "9862f001-eeca-486d-d4a0-4ab4c62f6e8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "và 14255\n",
            "của 13177\n",
            "là 9983\n",
            "có 9162\n",
            "được 9131\n",
            "trong 8654\n",
            "một 7575\n",
            "cho 7483\n",
            "với 7195\n",
            "không 6591\n",
            "các 6300\n",
            "người 6088\n",
            "khi 6011\n",
            "này 5301\n",
            "đến 5165\n",
            "để 5123\n",
            "đã 4431\n",
            "nhiều 4167\n",
            "trên 3842\n",
            "từ 3820\n",
            "vào 3617\n",
            "đó 3207\n",
            "những 3097\n",
            "ở 2943\n",
            "ra 2767\n",
            "tại 2756\n",
            "vn 2738\n",
            "lại 2673\n",
            "cũng 2622\n",
            "phải 2615\n",
            "còn 2589\n",
            "theo 2565\n",
            "nhưng 2545\n",
            "zing 2519\n",
            "về 2170\n",
            "sau 2047\n",
            "làm 1983\n",
            "lên 1921\n",
            "hơn 1755\n",
            "đây 1692\n",
            "năm 1654\n",
            "sẽ 1597\n",
            "chỉ 1569\n",
            "cả 1534\n",
            "cùng 1490\n",
            "ngày 1484\n",
            "như 1467\n",
            "mà 1458\n",
            "vẫn 1386\n",
            "đi 1373\n",
            "2 1358\n",
            "mới 1357\n",
            "khác 1300\n",
            "3 1296\n",
            "hai 1278\n",
            "qua 1250\n",
            "bạn 1138\n",
            "bên 1137\n",
            "1 1136\n",
            "khiến 1122\n",
            "5 1114\n",
            "lần 1051\n",
            "mình 1045\n",
            "lớn 1030\n",
            "bị 1022\n",
            "biết 1013\n",
            "trước 1000\n",
            "rất 991\n",
            "tới 968\n",
            "bằng 948\n",
            "mang 929\n",
            "nên 897\n",
            "4 896\n",
            "đang 870\n",
            "nước 863\n",
            "cách 863\n",
            "việt_nam 861\n",
            "đầu 849\n",
            "10 847\n",
            "việc 840\n",
            "nếu 835\n",
            "vừa 826\n",
            "thấy 824\n",
            "hàng 807\n",
            "vì 806\n",
            "ảnh 799\n",
            "đều 796\n",
            "nhau 788\n",
            "thời_gian 787\n",
            "sự 764\n",
            "anh 760\n",
            "6 737\n",
            "nhất 733\n",
            "ngoài 720\n",
            "điều 712\n",
            "hay 706\n",
            "giữa 699\n",
            "số 699\n",
            "từng 697\n",
            "thêm 692\n"
          ]
        }
      ],
      "source": [
        "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
        "total_label = 18\n",
        "vocab = {}\n",
        "label_vocab = {}\n",
        "for line in open('./data/news_categories.txt',encoding='utf8'):\n",
        "    words = line.split()\n",
        "    # lưu ý từ đầu tiên là nhãn\n",
        "    label = words[0]\n",
        "    if label not in label_vocab:\n",
        "        label_vocab[label] = {}\n",
        "    for word in words[1:]:\n",
        "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
        "        if word not in vocab:\n",
        "            vocab[word] = set()\n",
        "        vocab[word].add(label)\n",
        "\n",
        "count = {}\n",
        "for word in vocab:\n",
        "    if len(vocab[word]) == total_label:\n",
        "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
        "        \n",
        "sorted_count = sorted(count, key=count.get, reverse=True)\n",
        "for word in sorted_count[:100]:\n",
        "    print(word, count[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iV6dRNX6ysxW"
      },
      "outputs": [],
      "source": [
        "# loại stopword khỏi dữ liệu\n",
        "# lưu file dùng về sau\n",
        "stopword = set()\n",
        "with open('stopwords.txt', 'w',encoding='utf8') as fp:\n",
        "    for word in sorted_count[:100]:\n",
        "        stopword.add(word)\n",
        "        fp.write(word + '\\n')\n",
        "    \n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "    \n",
        "    \n",
        "with open('news_categories.prep', 'w',encoding='utf8') as fp:\n",
        "    for line in open('./data/news_categories.txt',encoding='utf8'):\n",
        "        line = remove_stopwords(line)\n",
        "        fp.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DmhHkG7zLMj",
        "outputId": "9f29a18f-eeac-4138-f03b-104ba109e84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__label__công_nghệ', '__label__du_lịch', '__label__giáo_dục', '__label__giải_trí', '__label__kinh_doanh', '__label__nhịp_sống', '__label__phim_ảnh', '__label__pháp_luật', '__label__sống_trẻ', '__label__sức_khỏe', '__label__thế_giới', '__label__thể_thao', '__label__thời_sự', '__label__thời_trang', '__label__xe_360', '__label__xuất_bản', '__label__âm_nhạc', '__label__ẩm_thực'] \n",
            "\n",
            "long_nhật chê chuyện phương_thanh cứu siu_black sao_việt cái nhìn ngược hoàn_toàn đông long_nhật rằng phương_thanh mọi chuyện rối_tung long_nhật chê chuyện phương_thanh cứu siu_black cái nhìn ngược hoàn_toàn đông long_nhật rằng phương_thanh mọi chuyện rối_tung chuyện siu_black vỡ_nợ thực_sự tạo cơn chấn_động giới giải_trí thay_vì máu_lửa sân_khấu giọng ca núi_rừng xuất_hiện gương_mặt bơ_phờ khóe miệng nở nụ_cười gượng_gạo phương_thanh duy_nhất phát_ngôn báo_chí thừa_nhận khoản nợ tỷ đồng siu_black hoàn_toàn im_lặng thay chị trả_lời tất_cả thắc_mắc giới truyền_thông chính nữ ca_sĩ hâm_mộ gọi cái tên chanh tin phương_thanh chấp_nhận đứng_mũi_chịu_sào thân nhìn chị đầy ái_ngại khen chị tốt_tính lúc khó_khăn ai ai bè cho_rằng giọng ca mèo hoang_dại quá chuyện nhảy chịu khổ ai đuổi suy_nghĩ đông cái nhìn hoạt_động giới showbiz nam ca_sĩ long_nhật đưa nhìn_nhận hoàn_toàn khác_biệt lời nam ca_sĩ huế thân_thiết chị siu sự_cố phương_thanh rối_tung mọi thứ chuyện phương_thanh nhận quản_lý siu_black trả_lời báo_chí chứ tin nóng như_thế làm_sao tôi bỏ_qua mọi nhìn_nhận hành_động nghĩa_hiệp thật_sự tôi nực_cười muốn đứng lo chuyện nợ_nần trước_hết tiền đủ sức trả nợ người_ta thì_thôi giúp giúp đừng nửa_chừng như_vậy ý phương_thanh tiền bản_thân cô ấy thời đỉnh_cao giờ cô ấy đóng phim kiếm tiền thù_lao đóng phim thấp lắm bộ phim 40 tập đóng tháng trời nhận khoản tiền ca_sĩ hát đêm ít_ra chị ấy dám đứng chịu_trận đâu ai gan chuyện vấn_đề nợ_nần tiền_bạc tiền giúp người_ta nói cái gì bây_giờ khả_năng sao nói tôi sẵn_sàng trả giúp siu_black siu hát trả nợ tôi như_thế chứ toàn nói chung_chung trả_lời rùm_beng báo_chí gì tổ nói nói lằng_nhằng mọi thứ thôi thương_siu như_thế mười_hại siu trả nợ siu phương_thanh khẳng_định giúp siu hát kiếm tiền một_cách đấy chứ chị siu bây_giờ nói rõ_ràng chị ấy tinh_thần hát siu_hát cháy hết_mình chị ấy hát cảm_xúc giờ tâm_trạng đâu cháy nợ_nần bủa_vây như_thế hay_là muốn hát dưới người_ta đòi nợ đông khán_giả tôi chắc_chắn cần bày treo băng_rôn chủ_nợ kéo liền ý phương_án hoàn_toàn hiệu_quả phương_thanh theo_đuổi việc_làm thực_tế cô ấy muốn anh_hùng muốn ra_tay nghĩa_hiệp càng sự_việc rối giờ phương_thanh bắt_đầu cảm_thấy mệt_mỏi muốn bỏ_của_chạy_lấy_người vậy gì giúp chị siu trường_hợp tôi thân_thiết quý_mến chị siu chúng_tôi mấy chục nay thời chưa nào nổi_tiếng tôi giờ muốn giúp siu tất_cả ngồi xuống bàn_bạc kỹ chứ không_thể sồn_sồn phương_thanh bấy_lâu_nay có_thể nghệ_sĩ đứng tổ_chức đêm nhạc bán vé lấy cát xê toàn_bộ tiền thu dùng ủng_hộ chị siu cần giúp_đỡ chính chân_thành tôi tin chị siu có_thể vượt khó_khăn trí_thức trẻ 3 \n",
            "\n",
            "thư_an_nguy gửi toàn_shinoda trái_tim muốn vỡ tung sống trẻ trưa hôm_nay 24 8 an_nguy đầu_tiên viết dòng tâm_sự cảm_xúc toàn_shinoda đột_ngột 27 7 an_nguy lập_tức quay mỹ vài dự tang_lễ toàn_shinoda đột_ngột bạn_trai an_nguy hoàn_toàn suy_sụp cô chưa nào tâm_sự mất_mát thời_điểm hiện_tại gần tháng toàn_shinoda mất an_nguy bộc_bạch cảm_xúc vui buồn tức_giận đau_khổ người_thân lặng nhìn toàn_shinoda giờ hỏa_táng trưa 27 7 linh_cữu vlogger toàn_shinoda di_chuyển đài_hóa_thân hoàn_vũ_văn_điển hà_nội 16h lễ hỏa_táng kết_thúc đau_thương lời tâm_sự an_nguy đăng_tải 30 phút thu_hút 80 000 lượt like thích ủng_hộ đồng_cảm cộng_đồng mạng an_nguy viết em giận em nghĩ giận đời chẳng bao_giờ tha_thứ em giận mãi im_lặng mãi chẳng dỗ em quay nữ hèn nữ lúc_nào sợ giận sợ thương em nữa nghe em chẳng tin ngàn em chẳng_thể tin chuyện gì xảy nửa vòng trái_đất em kẻ mất chân mất tay gọi chạy nhà em ngồi cái máy_tính nực_cười em trút giận tất_cả báo tin em em gào họ chừng nào gặp tin rồi họ gặp em ngồi đọc dòng chữ màn_hình đầu_tiên em cảm_thấy bất_lực thế cứ tuột dần khỏi tay em một_cách mơ_hồ em khóc em hiểu loại động_vật máu lạnh nào em em không_thể khóc em câm_lặng thôi chia_sẻ an_nguy facebook em kẻ hèn_nhát em muốn quay em sợ sợ khóc sợ người_ta thương_hại em em không_bao_giờ muốn ai thương_hại trên_hết em sợ đối_diện sự_thật sợ nhìn nằm em sợ em gọi không_bao_giờ nghe câu trả_lời nữa em tự_nhủ giấc_mơ giấc_mơ quá dài quá thật trở_về nghĩa_là tỉnh_giấc cơn ác_mộng trở_thành hiện_thực em suy_nghĩ ác_độc em nói em bảo_vệ bất_kì kẻ nào tổn_thương lợi_dụng động em thì động anh_em yên cười kệ rồi chúng_nó hiểu thôi à chúng_nó hiểu đâu rồi chúng_nó quyết chịu hiểu đâu em kệ em muốn giết chết kẻ thêu_dệt muốn tự tay giết hết chúng_nó súng vậy nhẹ_nhàng quá dao không_chỉ nhát chục trăm hình nữa em nói rồi cần em sợ bố con thằng nào hết em sống giận_dữ em giận ông trời giận giận giận tất_cả mọi xung_quanh giận ông trời sao đem mất giận sao bỏ mọi giận sao giữ người_ta viết em giận làm_sao viết cứ quay người_ta viết nữa em giận mấy hôm dám quên tức_giận vô_lý lúc_nào em muốn đập muốn phá tức_giận ấy đè nặng em kẻ nắm lấy trái_tim em bóp nghiến lấy nó em gì em sống cuộc_sống bình_thường ăn ngủ cười nói trái_tim muốn vỡ tung đêm ngồi dưới đường nhìn ban công phòng em gọi chờ tiếng xuống đợi một_tí em đợi mãi chẳng ai xuất_hiện em nghĩ rằng cần em đợi bao_lâu thôi em đợi rồi em nhận_ra cái bao_lâu ấy không_bao_giờ nữa em tiếp_tục mất thứ tốt_đẹp xảy có_điều mất mãi_mãi em tin kiếp kiếp thật em chắc_chắn gặp tiếp_tục em thành lâm_việt_anh việt lúc_nào lặng_lẽ quan_tâm giúp_đỡ bọn em bọn em đời quên vui_vẻ đau_đớn nhắc bây_giờ em ích_kỉ đòi nữa thanh_thản vướng_bận gì đừng lo em em yếu_đuối muốn mạnh_mẽ cần đường tình dang_dở an_nguy toàn_shinoda chính_thức công_khai tình_yêu tháng an_nguy gái toàn shinoda lặng_lẽ đột_ngột bạn_trai 8\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "for line in open('news_categories.prep',encoding='utf8'):\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('train.txt', 'w',encoding='utf8') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('test.txt', 'w',encoding='utf8') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(X_train[0], y_train[0], '\\n')\n",
        "print(X_test[0], y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "r192vzlM06g_"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"models\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5wx34bizQ8S",
        "outputId": "c835ab78-a686-424c-ded3-5aca549d3f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done training Naive Bayes in 12.60020637512207 seconds.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jLxhnyp70IWJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2MVw9vfzQ-5",
        "outputId": "43fe6b76-fd77-4315-fdc4-7e37fb4aab19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " __label__công_nghệ       0.91      0.92      0.92       532\n",
            "   __label__du_lịch       0.79      0.88      0.83       551\n",
            "  __label__giáo_dục       0.82      0.88      0.85       528\n",
            "  __label__giải_trí       0.59      0.74      0.66       487\n",
            "__label__kinh_doanh       0.78      0.84      0.81       498\n",
            " __label__nhịp_sống       0.85      0.49      0.62       497\n",
            "  __label__phim_ảnh       0.90      0.76      0.82       525\n",
            " __label__pháp_luật       0.90      0.92      0.91       543\n",
            "  __label__sống_trẻ       0.62      0.64      0.63       510\n",
            "  __label__sức_khỏe       0.79      0.88      0.83       496\n",
            "  __label__thế_giới       0.91      0.83      0.87       549\n",
            "  __label__thể_thao       0.95      0.95      0.95       508\n",
            "   __label__thời_sự       0.82      0.77      0.79       496\n",
            "__label__thời_trang       0.86      0.77      0.81       521\n",
            "    __label__xe_360       0.97      0.94      0.96       502\n",
            "  __label__xuất_bản       0.87      0.93      0.90       519\n",
            "   __label__âm_nhạc       0.83      0.87      0.85       554\n",
            "   __label__ẩm_thực       0.90      0.94      0.92       520\n",
            "\n",
            "           accuracy                           0.83      9336\n",
            "          macro avg       0.84      0.83      0.83      9336\n",
            "       weighted avg       0.84      0.83      0.83      9336\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-GATp1orz6uB"
      },
      "outputs": [],
      "source": [
        "# xem kết quả cho  văn bản model naive bayes đã load ở trên\n",
        "def predict(document):\n",
        "# document = '''Do CLB Quảng Ninh không đưa ra được kế hoạch cụ thể cho V-League 2021, HLV Phan Thanh Hùng quyết định kết thúc hợp đồng sớm một năm.\n",
        "\n",
        "# \"Tôi vừa lấy giấy thanh lý hợp đồng, chính thức chia tay Quảng Ninh\", HLV Phan Thanh Hùng nói với VnExpress sáng 15/12. \n",
        "# \"Tôi có tình cảm với đội bóng cũng như người hâm mộ nơi đây. Nhưng, tôi thấy mình không còn phù hợp với CLB nên nghỉ. \n",
        "# Trước mắt, tôi về nhà, chưa nhận lời dẫn dắt đội bóng nào'''\n",
        "\n",
        "  document = text_preprocess(document)\n",
        "  document = remove_stopwords(document)\n",
        "\n",
        "  label = nb_model.predict([document])\n",
        "  print('Predict label:', label_encoder.inverse_transform(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RlY2BBpzRCE",
        "outputId": "a06b0780-b797-4a0f-9b4a-5f483e6c4f65"
      },
      "outputs": [],
      "source": [
        "document = '''Do CLB Quảng Ninh không đưa ra được kế hoạch cụ thể cho V-League 2021, HLV Phan Thanh Hùng quyết định kết thúc hợp đồng sớm một năm.\n",
        "\n",
        "\"Tôi vừa lấy giấy thanh lý hợp đồng, chính thức chia tay Quảng Ninh\", HLV Phan Thanh Hùng nói với VnExpress sáng 15/12. \n",
        "\"Tôi có tình cảm với đội bóng cũng như người hâm mộ nơi đây. Nhưng, tôi thấy mình không còn phù hợp với CLB nên nghỉ. \n",
        "Trước mắt, tôi về nhà, chưa nhận lời dẫn dắt đội bóng nào'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predict label: ['__label__thể_thao']\n"
          ]
        }
      ],
      "source": [
        "predict(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njh_zfj0LAOx",
        "outputId": "1ed6fdda-9d53-4e16-f933-fb7f4e6e09d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done training Linear Classifier in 189.74464750289917 seconds.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tôi yêu việt nam quê_hương tôi\n"
          ]
        }
      ],
      "source": [
        "document = text_preprocess('Tôi yêu Việt Nam quê hương tôi')\n",
        "print(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyvi import ViTokenizer\n",
        "a = ViTokenizer.tokenize('Tôi học ngành khoa học  tại trường Đại học Quy Nhơn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tôi học ngành CNTT tại trường Đại_học Quy_Nhơn'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PhanLoaiChuDe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
